{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled1.ipynb","provenance":[],"authorship_tag":"ABX9TyOBU5USfZybA1ubsz889RMz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1333d0fff35c47569dd9d204603a04a2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_77084be0a2c44ee891c5eabeb2eb2ee0","IPY_MODEL_e91d4a04faa147b09c8352712bea7f35","IPY_MODEL_c452295eae2d455b9c2ccfa6b8ba5e54"],"layout":"IPY_MODEL_5ee81cf5ca2e4f249c0be66ccf67d8a2"}},"77084be0a2c44ee891c5eabeb2eb2ee0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_058401ce26cf4dbbbea7cca77a2034b1","placeholder":"​","style":"IPY_MODEL_1ccc00508ed749dc97819790996ad477","value":"Downloading: 100%"}},"e91d4a04faa147b09c8352712bea7f35":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9bc6b9a935a4bf082500fe6e763906a","max":536063208,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1af61480b7db4764b9be03841120a09c","value":536063208}},"c452295eae2d455b9c2ccfa6b8ba5e54":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_32d40148185f4dc4a9b527ecd9c9043e","placeholder":"​","style":"IPY_MODEL_650b46fe1e6347b0b5db3972e80b3dfd","value":" 511M/511M [00:15&lt;00:00, 43.3MB/s]"}},"5ee81cf5ca2e4f249c0be66ccf67d8a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"058401ce26cf4dbbbea7cca77a2034b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ccc00508ed749dc97819790996ad477":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f9bc6b9a935a4bf082500fe6e763906a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1af61480b7db4764b9be03841120a09c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"32d40148185f4dc4a9b527ecd9c9043e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"650b46fe1e6347b0b5db3972e80b3dfd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"xDY4d3c9_pm3"},"outputs":[],"source":["import tensorflow_datasets as tfds\n","import tensorflow as tf"]},{"cell_type":"code","source":["(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n","          split = (tfds.Split.TRAIN, tfds.Split.TEST),\n","          as_supervised=True,\n","          with_info=True)"],"metadata":{"id":"wozuEdzJ_xtu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for review, label in tfds.as_numpy(ds_train.take(5)):\n","    print(review.decode()[0:50], '\\t', label)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"icIML7nZ__Fw","executionInfo":{"status":"ok","timestamp":1653395581159,"user_tz":-60,"elapsed":8,"user":{"displayName":"Wale Abiodun Ogundeji","userId":"05118496143102978176"}},"outputId":"a165a4f6-7fca-4f9e-a71e-6abf831da8d1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["This was an absolutely terrible movie. Don't be lu \t 0\n","I have been known to fall asleep during films, but \t 0\n","Mann photographs the Alberta Rocky Mountains in a  \t 0\n","This is the kind of film for a snowy Sunday aftern \t 1\n","As others have mentioned, all the women that go nu \t 1\n"]}]},{"cell_type":"code","source":["!pip install -q transformers"],"metadata":{"id":"yMYpffK0BQSR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertTokenizer"],"metadata":{"id":"2d0D4FOhBVve"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"],"metadata":{"id":"Y9n1vKZCBX9_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def convert_example_to_feature(review):\n","  return tokenizer.encode_plus(review,\n","                add_special_tokens = True, # add [CLS], [SEP]\n","                max_length = max_length, # max length of the text that can go to BERT\n","                pad_to_max_length = True, # add [PAD] tokens\n","                return_attention_mask = True, # add attention mask to not focus on pad tokens\n","              )"],"metadata":{"id":"nOgqFJvWBYIk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# can be up to 512 for BERT\n","max_length = 512\n","batch_size = 6"],"metadata":{"id":"Ds12HzNUBn72"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n","  return {\n","      \"input_ids\": input_ids,\n","      \"token_type_ids\": token_type_ids,\n","      \"attention_mask\": attention_masks,\n","  }, label"],"metadata":{"id":"b8pHxjH4B9EI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def encode_examples(ds, limit=-1):\n","  # prepare list, so that we can build up final TensorFlow dataset from slices.\n","  input_ids_list = []\n","  token_type_ids_list = []\n","  attention_mask_list = []\n","  label_list = []\n","  if (limit > 0):\n","      ds = ds.take(limit)\n","  for review, label in tfds.as_numpy(ds):\n","    bert_input = convert_example_to_feature(review.decode())\n","    input_ids_list.append(bert_input['input_ids'])\n","    token_type_ids_list.append(bert_input['token_type_ids'])\n","    attention_mask_list.append(bert_input['attention_mask'])\n","    label_list.append([label])\n","  return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)"],"metadata":{"id":"7-ZvGbrBCIcV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train dataset\n","ds_train_encoded = encode_examples(ds_train).shuffle(10000).batch(batch_size)\n","# test dataset\n","ds_test_encoded = encode_examples(ds_test).batch(batch_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2UV9RJ6ECcEu","executionInfo":{"status":"ok","timestamp":1653396183467,"user_tz":-60,"elapsed":593030,"user":{"displayName":"Wale Abiodun Ogundeji","userId":"05118496143102978176"}},"outputId":"d29fba3a-0b56-400b-a916-bb8ae288106c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["from transformers import TFBertForSequenceClassification\n","import tensorflow as tf\n","# recommended learning rate for Adam 5e-5, 3e-5, 2e-5\n","learning_rate = 2e-5\n","# we will do just 1 epoch, though multiple epochs might be better as long as we will not overfit the model\n","number_of_epochs = 1\n","# model initialization\n","model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":118,"referenced_widgets":["1333d0fff35c47569dd9d204603a04a2","77084be0a2c44ee891c5eabeb2eb2ee0","e91d4a04faa147b09c8352712bea7f35","c452295eae2d455b9c2ccfa6b8ba5e54","5ee81cf5ca2e4f249c0be66ccf67d8a2","058401ce26cf4dbbbea7cca77a2034b1","1ccc00508ed749dc97819790996ad477","f9bc6b9a935a4bf082500fe6e763906a","1af61480b7db4764b9be03841120a09c","32d40148185f4dc4a9b527ecd9c9043e","650b46fe1e6347b0b5db3972e80b3dfd"]},"id":"-ssAM92mCkRf","executionInfo":{"status":"ok","timestamp":1653396203853,"user_tz":-60,"elapsed":20409,"user":{"displayName":"Wale Abiodun Ogundeji","userId":"05118496143102978176"}},"outputId":"a6952bab-0454-4934-e97b-1e66709328bf"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/511M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1333d0fff35c47569dd9d204603a04a2"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["# choosing Adam optimizer\n","optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n","# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n","model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"],"metadata":{"id":"qcikQYS6GQkO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bert_history = model.fit(ds_train_encoded, epochs=number_of_epochs, validation_data=ds_test_encoded)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rSfwgKNYGYql","outputId":"dd74fe99-cb16-43b9-c7c5-e42aa87d0aec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" 661/4167 [===>..........................] - ETA: 33:35:20 - loss: 0.3526 - accuracy: 0.8376"]}]},{"cell_type":"code","source":["test_sentence = \"This is a really good movie. I loved it and will watch again\"\n","\n","predict_input = tokenizer.encode(test_sentence, truncation=True, padding=True, return_tensors=\"tf\")"],"metadata":{"id":"GOghle-mGkpV"},"execution_count":null,"outputs":[]}]}